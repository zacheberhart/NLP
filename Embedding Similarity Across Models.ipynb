{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os, pickle, sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from random import sample, shuffle\n",
    "from math import ceil\n",
    "\n",
    "# EMBEDDINGS\n",
    "from sklearn.metrics.pairwise import cosine_similarity # ALL\n",
    "from bert_serving.client import BertClient             # BERT\n",
    "from bert_serving.server.graph import optimize_graph   # BERT\n",
    "from bert_serving.server.helper import get_args_parser # BERT\n",
    "import spacy                                           # GLoVe\n",
    "import tensorflow as tf                                # USE & BERT\n",
    "import tensorflow_hub as hub                           # USE\n",
    "from gensim.test.utils import datapath                 # Word2Vec & FastText\n",
    "from gensim.models import KeyedVectors                 # Word2Vec\n",
    "from gensim.models.fasttext import load_facebook_vectors, load_facebook_model # FastText\n",
    "\n",
    "DB_PATH = 'data/db/spotify.db'\n",
    "SEED = 413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Similarity Across Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following notebook includes Semantic Similarity comparisons between several different models**:\n",
    " - BERT\n",
    " - Universal Sentence Encoder\n",
    " - Word2Vec\n",
    " - GLoVe\n",
    " - FastText\n",
    "\n",
    "The Semantic Similarity scores are will be used to evaluate whether or not a playlist name, returned by Spotify's API, is relevant to the search query which was used. This is part of a larger project to build a new type of spectrogram.\n",
    "\n",
    "**Data to Generate**:\n",
    " - Pln Similarity:\n",
    "     - Max of each query\n",
    "     - Avg of each query\n",
    " - Max Token Similarity:\n",
    "     - Max of each query against each pln token\n",
    "     - Avg of max of each query against each pln token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_operators(query: str):\n",
    "    '''Convert query into a list, splitting where there is a Spotify search operator'''\n",
    "    if 'NOT' in query:\n",
    "        return [query.split(' NOT ')[0]]\n",
    "    elif 'OR' in query:\n",
    "        return query.split(' OR ')\n",
    "    elif 'AND' in query:\n",
    "        return query.split(' AND ')\n",
    "    return [query]\n",
    "\n",
    "def get_pln_similarity(queries: list, pln: str, row_idx: int, sim_func):\n",
    "    '''Get the similarity between the query (or queries if an operator was used) and the playlist name.'''\n",
    "    if row_idx % 10000 == 0: print(f'Row: {row_idx}')\n",
    "    try:\n",
    "        sims = [sim_func(q, pln) for q in queries]\n",
    "        return max(sims), np.mean(sims)\n",
    "    except ValueError as e:\n",
    "        print(f'ValueError: {e}')\n",
    "        return np.nan\n",
    "    except KeyError as e:\n",
    "        print(f'KeyError: {e}')\n",
    "        return np.nan\n",
    "\n",
    "def get_max_token_similarity(queries: list, pln: str, row_idx: int, sim_func):\n",
    "    '''Get the max similarities of the query (or queries if operator was used) compared to each token of the pln.'''\n",
    "    if row_idx % 10000 == 0: print(f'Row: {row_idx}')\n",
    "\n",
    "    # check if q|pln is in q|pln\n",
    "    for q in queries:\n",
    "        if   q in pln: return 1., 1.\n",
    "        elif pln in q: return 1., 1.\n",
    "\n",
    "    # return the max of the similarity for each token\n",
    "    try:\n",
    "        sim_max = [max([sim_func(q, token) for token in pln.split()]) for q in queries]\n",
    "        return max(sim_max), np.mean(sim_max)\n",
    "    except ValueError as e:\n",
    "        print(f'ValueError: {e}')\n",
    "        return np.nan\n",
    "    except KeyError as e:\n",
    "        print(f'KeyError: {e}')\n",
    "        return np.nan\n",
    "\n",
    "def add_model_avg_similarity(df, model: str):\n",
    "    '''Calculate the averages between similarity scores for a given model'''\n",
    "    tdf = df[[col for col in df.columns if f'{model}_' in col]]\n",
    "    df[f'{model}_overall_avg_max_similarity'] = (tdf[f'{model}_max_pln_similarity'] +\n",
    "                                                 tdf[f'{model}_max_token_max_similarity']) / 2\n",
    "    df[f'{model}_overall_avg_avg_similarity'] = (tdf[f'{model}_avg_pln_similarity'] +\n",
    "                                                 tdf[f'{model}_avg_token_max_similarity']) / 2\n",
    "    df[f'{model}_overall_similarity']          = tdf.mean(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('processed_batch_uniq_plns_spec_char_edit')\n",
    "df = deepcopy(df[df.playlist_name.str.len() > 0])\n",
    "df['proc_query'] = df['query'].apply(process_query_operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tokens Count:           96923\n",
      "All Queries & PLNs Count:  540612\n"
     ]
    }
   ],
   "source": [
    "# get unique queries and plns from data\n",
    "queries = df['query'].unique().tolist()\n",
    "plns = df.playlist_name.unique().tolist()\n",
    "\n",
    "# process plns with search operators\n",
    "proc_queries = []\n",
    "for q in queries:\n",
    "    q = process_query_operators(q)\n",
    "    if type(q) == list: proc_queries += q\n",
    "    else: proc_queries.append(q)\n",
    "\n",
    "# dedup\n",
    "queries = list(set(proc_queries))\n",
    "del proc_queries\n",
    "\n",
    "# add to single list for quicker processing\n",
    "all_queries_and_plns = queries + plns\n",
    "\n",
    "# get pln tokens\n",
    "for pln in plns:\n",
    "    if ' ' in pln:\n",
    "        all_queries_and_plns += pln.split()\n",
    "\n",
    "# dedup\n",
    "all_queries_and_plns = list(set(all_queries_and_plns))\n",
    "all_tokens = list(set([t for tt in [token.split() for token in all_queries_and_plns] for t in tt]))\n",
    "\n",
    "# pickle for BERT\n",
    "# with open('all_queries_and_plns_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_queries_and_plns, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f'All Tokens Count:           {len(all_tokens)}')\n",
    "print(f'All Queries & PLNs Count:  {len(all_queries_and_plns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BERT Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BERT & TensorFlow`\n",
    "\n",
    "Note: Using the client/server model provided by `bert-as-a-service` was good for one off tasks, but was incredibly slow for this use case. To remedy that, I exported the TennsorFlow graph and generate embeddings for each of the token and documents (in this case, `playlist_name`) and then calculated similiarty here.\n",
    "\n",
    "To see how the embeddings were generated, see the `BERT Embeddings (on GPU)` notebook in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_doc_similarity(query: str, pln: str):\n",
    "    '''Calculate similarity using BERT using the document embeddings (SEQ_LEN = 22)'''\n",
    "    return cosine_similarity(bert_doc_embeds[query], bert_doc_embeds[pln])[0][0]\n",
    "\n",
    "def get_bert_token_similarity(query: str, pln: str):\n",
    "    '''Calculate similarity using BERT using the token embeddings (min, SEQ_LEN = 4)'''\n",
    "    return cosine_similarity(bert_doc_embeds[query], bert_token_embeds[pln])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_doc_embeds.pkl', 'rb') as f:\n",
    "    bert_doc_embeds = pickle.load(f)\n",
    "\n",
    "with open('bert_token_embeds.pkl', 'rb') as f:\n",
    "    bert_token_embeds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>track_id</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>search_rank</th>\n",
       "      <th>proc_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154108</th>\n",
       "      <td>happy hardcore</td>\n",
       "      <td>7brm4QECDelvH883WSJZgK</td>\n",
       "      <td>happy hardcore</td>\n",
       "      <td>448</td>\n",
       "      <td>[happy hardcore]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353278</th>\n",
       "      <td>gleeful</td>\n",
       "      <td>4hBW3h6FnQNh7NRmyxLLG7</td>\n",
       "      <td>gleeful</td>\n",
       "      <td>70</td>\n",
       "      <td>[gleeful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003741</th>\n",
       "      <td>introspection</td>\n",
       "      <td>1yepm3wDAHedCHhxVKzTtL</td>\n",
       "      <td>introspective</td>\n",
       "      <td>1504</td>\n",
       "      <td>[introspection]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533284</th>\n",
       "      <td>silly</td>\n",
       "      <td>4Km5HrUvYTaSUfiSGPJeQR</td>\n",
       "      <td>silly salmon</td>\n",
       "      <td>562</td>\n",
       "      <td>[silly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227253</th>\n",
       "      <td>pop</td>\n",
       "      <td>09IStsImFySgyp0pIQdqAc</td>\n",
       "      <td>clean pop playlist</td>\n",
       "      <td>334</td>\n",
       "      <td>[pop]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  query                track_id       playlist_name  \\\n",
       "1154108  happy hardcore  7brm4QECDelvH883WSJZgK      happy hardcore   \n",
       "353278          gleeful  4hBW3h6FnQNh7NRmyxLLG7             gleeful   \n",
       "1003741   introspection  1yepm3wDAHedCHhxVKzTtL       introspective   \n",
       "533284            silly  4Km5HrUvYTaSUfiSGPJeQR        silly salmon   \n",
       "227253              pop  09IStsImFySgyp0pIQdqAc  clean pop playlist   \n",
       "\n",
       "         search_rank        proc_query  \n",
       "1154108          448  [happy hardcore]  \n",
       "353278            70         [gleeful]  \n",
       "1003741         1504   [introspection]  \n",
       "533284           562           [silly]  \n",
       "227253           334             [pop]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bert_max_pln_similarity'], df['bert_avg_pln_similarity'] = zip(\n",
    "    *df.apply(lambda row: get_pln_similarity(row['proc_query'],\n",
    "                                             row['playlist_name'],\n",
    "                                             row.name,\n",
    "                                             get_bert_doc_similarity\n",
    "                                            ), axis = 1)\n",
    ")\n",
    "\n",
    "df['bert_max_token_max_similarity'], df['bert_avg_token_max_similarity'] = zip(\n",
    "    *df.apply(lambda row: get_max_token_similarity(row['proc_query'],\n",
    "                                                   row['playlist_name'],\n",
    "                                                   row.name,\n",
    "                                                   get_bert_token_similarity\n",
    "                                                  ), axis = 1)\n",
    ")\n",
    "\n",
    "df = add_model_avg_similarity(df, 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('proc_query', 1).reset_index(drop = True).to_feather('final_uniq_plns_WITH_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1402119, 12)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder\n",
    "\n",
    "Reference: http://www.nishanpantha.com.np/programming/universal-sentence-encoder-semantic-search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_USE_embeddings(text, embed_model):\n",
    "    if type(text) is str:\n",
    "        text = [text]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        return sess.run(embed_model(text))\n",
    "\n",
    "def get_emb_dict(word_list, model):\n",
    "    return {w: e.reshape(1, -1) for w, e in zip(word_list, get_USE_embeddings(word_list, model))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings\n",
    "\n",
    "**Note**: USE Embeddings are quite memory intensive, unless you are working on a remote machine then you'll probably only be able to load one model at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAN_module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "TRANS_module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'transformer'\n",
    "#MODEL = 'dan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model\n",
    "if MODEL == 'transformer':\n",
    "    embed = hub.Module(TRANS_module_url)\n",
    "    write_fname = 'guse_all-data_new-spec-char_transformer.pkl'\n",
    "\n",
    "if MODEL == 'dan':\n",
    "    embed = hub.Module(DAN_module_url)\n",
    "    write_fname = 'guse_all-data_new-spec-char_dan.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 100\n",
    "batch_size = len(all_queries_and_plns) // batches\n",
    "\n",
    "saved_emb = {}\n",
    "\n",
    "for i in range(batches):\n",
    "    \n",
    "    print(f'Batch: {i+1}')\n",
    "    print(f'Saved Emb Size: {len(saved_emb)}')\n",
    "    print()\n",
    "    \n",
    "    batch = list(all_queries_and_plns[batch_size*i:batch_size*(i+1)])\n",
    "    saved_emb = {**saved_emb, **get_emb_dict(batch, embed)}\n",
    "    \n",
    "    with open(write_fname, 'wb') as f:\n",
    "        pickle.dump(saved_emb, f, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_use_transformer_similarity(query: str, pln: str):\n",
    "    '''Calculate similarity using USE using the Transformer embeddings'''\n",
    "    return cosine_similarity(emb_t[query], emb_t[pln])[0][0]\n",
    "\n",
    "def get_use_dan_similarity(query: str, pln: str):\n",
    "    '''Calculate similarity using USE using the DAN embeddings'''\n",
    "    return cosine_similarity(emb_d[query], emb_d[pln])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('guse_all-data_new-spec-char_transformer.pkl', 'rb') as f:\n",
    "    emb_t = pickle.load(f)\n",
    "    \n",
    "with open('guse_all-data_new-spec-char_dan.pkl', 'rb') as f:\n",
    "    emb_d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>track_id</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>search_rank</th>\n",
       "      <th>bert_max_pln_similarity</th>\n",
       "      <th>bert_avg_pln_similarity</th>\n",
       "      <th>bert_max_token_max_similarity</th>\n",
       "      <th>bert_avg_token_max_similarity</th>\n",
       "      <th>bert_overall_avg_max_similarity</th>\n",
       "      <th>bert_overall_avg_avg_similarity</th>\n",
       "      <th>bert_overal_similarity</th>\n",
       "      <th>proc_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1036008</th>\n",
       "      <td>smoke</td>\n",
       "      <td>0TCnOEVeLQMXOUrpPlM7uY</td>\n",
       "      <td>when you high</td>\n",
       "      <td>519</td>\n",
       "      <td>0.848013</td>\n",
       "      <td>0.848013</td>\n",
       "      <td>0.892566</td>\n",
       "      <td>0.892566</td>\n",
       "      <td>0.870289</td>\n",
       "      <td>0.870289</td>\n",
       "      <td>0.870289</td>\n",
       "      <td>[smoke]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004237</th>\n",
       "      <td>melancholic</td>\n",
       "      <td>7wwp2dzuw5SyB5Up6Gf5O2</td>\n",
       "      <td>melancholic chill</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.938558</td>\n",
       "      <td>0.938558</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969279</td>\n",
       "      <td>0.969279</td>\n",
       "      <td>0.969279</td>\n",
       "      <td>[melancholic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30374</th>\n",
       "      <td>deep house</td>\n",
       "      <td>61HZRStXWgL7CXkw3yW2rv</td>\n",
       "      <td>em casa sunset</td>\n",
       "      <td>966</td>\n",
       "      <td>0.889679</td>\n",
       "      <td>0.889679</td>\n",
       "      <td>0.855499</td>\n",
       "      <td>0.855499</td>\n",
       "      <td>0.872589</td>\n",
       "      <td>0.872589</td>\n",
       "      <td>0.872589</td>\n",
       "      <td>[deep house]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               query                track_id      playlist_name  search_rank  \\\n",
       "1036008        smoke  0TCnOEVeLQMXOUrpPlM7uY      when you high          519   \n",
       "1004237  melancholic  7wwp2dzuw5SyB5Up6Gf5O2  melancholic chill         1095   \n",
       "30374     deep house  61HZRStXWgL7CXkw3yW2rv     em casa sunset          966   \n",
       "\n",
       "         bert_max_pln_similarity  bert_avg_pln_similarity  \\\n",
       "1036008                 0.848013                 0.848013   \n",
       "1004237                 0.938558                 0.938558   \n",
       "30374                   0.889679                 0.889679   \n",
       "\n",
       "         bert_max_token_max_similarity  bert_avg_token_max_similarity  \\\n",
       "1036008                       0.892566                       0.892566   \n",
       "1004237                       1.000000                       1.000000   \n",
       "30374                         0.855499                       0.855499   \n",
       "\n",
       "         bert_overall_avg_max_similarity  bert_overall_avg_avg_similarity  \\\n",
       "1036008                         0.870289                         0.870289   \n",
       "1004237                         0.969279                         0.969279   \n",
       "30374                           0.872589                         0.872589   \n",
       "\n",
       "         bert_overal_similarity     proc_query  \n",
       "1036008                0.870289        [smoke]  \n",
       "1004237                0.969279  [melancholic]  \n",
       "30374                  0.872589   [deep house]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_models = {'transformer': get_use_transformer_similarity,\n",
    "              'dan': get_use_dan_similarity\n",
    "             }\n",
    "\n",
    "for model_name, use_sim_func in use_models.items():\n",
    "    \n",
    "    print(f'\\nAdding pln_similarity for: {model_name}\\n')\n",
    "    df[f'use_{model_name}_max_pln_similarity'], df[f'use_{model_name}_avg_pln_similarity'] = zip(\n",
    "        *df.apply(lambda row: get_pln_similarity(row['proc_query'],\n",
    "                                                 row['playlist_name'],\n",
    "                                                 row.name,\n",
    "                                                 use_sim_func,\n",
    "                                                ), axis = 1)\n",
    "    )\n",
    "    \n",
    "    print(f'\\nAdding token_max_similarity for: {model_name}\\n')\n",
    "    df[f'use_{model_name}_max_token_max_similarity'], df[f'use_{model_name}_avg_token_max_similarity'] = zip(\n",
    "        *df.apply(lambda row: get_max_token_similarity(row['proc_query'],\n",
    "                                                       row['playlist_name'],\n",
    "                                                       row.name,\n",
    "                                                       use_sim_func,\n",
    "                                                      ), axis = 1)\n",
    "    )\n",
    "    \n",
    "    print(f'\\nAdding overall_similarity for: {model_name}\\n')\n",
    "    df = add_model_avg_similarity(df, f'use_{model_name}')\n",
    "\n",
    "# add extra avg overall col for use since there are two different models\n",
    "df['use_overall_similarity'] = (df.use_transformer_overall_avg_avg_similarity +\n",
    "                                df.use_dan_overall_avg_avg_similarity) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>track_id</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>search_rank</th>\n",
       "      <th>bert_max_pln_similarity</th>\n",
       "      <th>bert_avg_pln_similarity</th>\n",
       "      <th>bert_max_token_max_similarity</th>\n",
       "      <th>bert_avg_token_max_similarity</th>\n",
       "      <th>bert_overall_avg_max_similarity</th>\n",
       "      <th>bert_overall_avg_avg_similarity</th>\n",
       "      <th>...</th>\n",
       "      <th>use_transformer_overall_avg_avg_similarity</th>\n",
       "      <th>use_transformer_overall_similarity</th>\n",
       "      <th>use_dan_max_pln_similarity</th>\n",
       "      <th>use_dan_avg_pln_similarity</th>\n",
       "      <th>use_dan_max_token_max_similarity</th>\n",
       "      <th>use_dan_avg_token_max_similarity</th>\n",
       "      <th>use_dan_overall_avg_max_similarity</th>\n",
       "      <th>use_dan_overall_avg_avg_similarity</th>\n",
       "      <th>use_dan_overall_similarity</th>\n",
       "      <th>use_overall_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1288606</th>\n",
       "      <td>sport OR sports</td>\n",
       "      <td>2LPUvD5DDOO4UYGkWgjI2C</td>\n",
       "      <td>sport</td>\n",
       "      <td>1109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960884</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969096</td>\n",
       "      <td>0.984548</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919925</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959963</td>\n",
       "      <td>0.979981</td>\n",
       "      <td>0.964530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437517</th>\n",
       "      <td>commute</td>\n",
       "      <td>1etiUDkISHELzQGMY79ryt</td>\n",
       "      <td>commute</td>\n",
       "      <td>1032</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637871</th>\n",
       "      <td>unforgettable</td>\n",
       "      <td>2xjMTChMsBkonVJJtO2kZZ</td>\n",
       "      <td>the best of opera</td>\n",
       "      <td>75</td>\n",
       "      <td>0.791004</td>\n",
       "      <td>0.791004</td>\n",
       "      <td>0.78657</td>\n",
       "      <td>0.78657</td>\n",
       "      <td>0.788787</td>\n",
       "      <td>0.788787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556367</td>\n",
       "      <td>0.556367</td>\n",
       "      <td>0.445041</td>\n",
       "      <td>0.445041</td>\n",
       "      <td>0.63021</td>\n",
       "      <td>0.63021</td>\n",
       "      <td>0.537625</td>\n",
       "      <td>0.537625</td>\n",
       "      <td>0.537625</td>\n",
       "      <td>0.546996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   query                track_id      playlist_name  \\\n",
       "1288606  sport OR sports  2LPUvD5DDOO4UYGkWgjI2C              sport   \n",
       "437517           commute  1etiUDkISHELzQGMY79ryt            commute   \n",
       "637871     unforgettable  2xjMTChMsBkonVJJtO2kZZ  the best of opera   \n",
       "\n",
       "         search_rank  bert_max_pln_similarity  bert_avg_pln_similarity  \\\n",
       "1288606         1109                 1.000000                 0.960884   \n",
       "437517          1032                 1.000000                 1.000000   \n",
       "637871            75                 0.791004                 0.791004   \n",
       "\n",
       "         bert_max_token_max_similarity  bert_avg_token_max_similarity  \\\n",
       "1288606                        1.00000                        1.00000   \n",
       "437517                         1.00000                        1.00000   \n",
       "637871                         0.78657                        0.78657   \n",
       "\n",
       "         bert_overall_avg_max_similarity  bert_overall_avg_avg_similarity  \\\n",
       "1288606                         1.000000                         0.980442   \n",
       "437517                          1.000000                         1.000000   \n",
       "637871                          0.788787                         0.788787   \n",
       "\n",
       "         ...  use_transformer_overall_avg_avg_similarity  \\\n",
       "1288606  ...                                    0.969096   \n",
       "437517   ...                                    1.000000   \n",
       "637871   ...                                    0.556367   \n",
       "\n",
       "        use_transformer_overall_similarity  use_dan_max_pln_similarity  \\\n",
       "1288606                           0.984548                    1.000000   \n",
       "437517                            1.000000                    1.000000   \n",
       "637871                            0.556367                    0.445041   \n",
       "\n",
       "         use_dan_avg_pln_similarity  use_dan_max_token_max_similarity  \\\n",
       "1288606                    0.919925                           1.00000   \n",
       "437517                     1.000000                           1.00000   \n",
       "637871                     0.445041                           0.63021   \n",
       "\n",
       "         use_dan_avg_token_max_similarity  use_dan_overall_avg_max_similarity  \\\n",
       "1288606                           1.00000                            1.000000   \n",
       "437517                            1.00000                            1.000000   \n",
       "637871                            0.63021                            0.537625   \n",
       "\n",
       "         use_dan_overall_avg_avg_similarity  use_dan_overall_similarity  \\\n",
       "1288606                            0.959963                    0.979981   \n",
       "437517                             1.000000                    1.000000   \n",
       "637871                             0.537625                    0.537625   \n",
       "\n",
       "         use_overall_similarity  \n",
       "1288606                0.964530  \n",
       "437517                 1.000000  \n",
       "637871                 0.546996  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('proc_query', 1).reset_index(drop = True).to_feather('final_uniq_plns_WITH_BERT&USE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec doesn't support OOV tokens so, depending on your use case, they might not be that helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_similarity(query: str, pln: str):\n",
    "    '''Calculate similarity using Word2Vec using Gensim'''\n",
    "    return m.similarity(query, pln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = datapath('word2vec_pretrain/GoogleNews-vectors-negative300.bin')\n",
    "m = KeyedVectors.load_word2vec_format(w2v_path, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>track_id</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>search_rank</th>\n",
       "      <th>bert_max_pln_similarity</th>\n",
       "      <th>bert_avg_pln_similarity</th>\n",
       "      <th>bert_max_token_max_similarity</th>\n",
       "      <th>bert_avg_token_max_similarity</th>\n",
       "      <th>bert_overall_avg_max_similarity</th>\n",
       "      <th>bert_overall_avg_avg_similarity</th>\n",
       "      <th>...</th>\n",
       "      <th>ft_wiki_overall_avg_avg_similarity</th>\n",
       "      <th>ft_wiki_overall_similarity</th>\n",
       "      <th>ft_crawl_max_pln_similarity</th>\n",
       "      <th>ft_crawl_avg_pln_similarity</th>\n",
       "      <th>ft_crawl_max_token_max_similarity</th>\n",
       "      <th>ft_crawl_avg_token_max_similarity</th>\n",
       "      <th>ft_crawl_overall_avg_max_similarity</th>\n",
       "      <th>ft_crawl_overall_avg_avg_similarity</th>\n",
       "      <th>ft_crawl_overall_similarity</th>\n",
       "      <th>proc_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>558968</th>\n",
       "      <td>motorcycle</td>\n",
       "      <td>60SdxE8apGAxMiRrpbmLY0</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>1031</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[motorcycle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309140</th>\n",
       "      <td>charts</td>\n",
       "      <td>4S8d14HvHb70ImctNgVzQQ</td>\n",
       "      <td>charts</td>\n",
       "      <td>4762</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[charts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069690</th>\n",
       "      <td>90s OR nineties</td>\n",
       "      <td>19YkX6WwgFKwCQ5edvLWvn</td>\n",
       "      <td>rock nacional anos 90 anos 2000</td>\n",
       "      <td>1143</td>\n",
       "      <td>0.74047</td>\n",
       "      <td>0.690026</td>\n",
       "      <td>0.841927</td>\n",
       "      <td>0.770233</td>\n",
       "      <td>0.791198</td>\n",
       "      <td>0.73013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109992</td>\n",
       "      <td>0.196221</td>\n",
       "      <td>0.319526</td>\n",
       "      <td>0.258843</td>\n",
       "      <td>0.566329</td>\n",
       "      <td>0.553025</td>\n",
       "      <td>0.442928</td>\n",
       "      <td>0.405934</td>\n",
       "      <td>0.424431</td>\n",
       "      <td>[90s, nineties]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   query                track_id  \\\n",
       "558968        motorcycle  60SdxE8apGAxMiRrpbmLY0   \n",
       "1309140           charts  4S8d14HvHb70ImctNgVzQQ   \n",
       "1069690  90s OR nineties  19YkX6WwgFKwCQ5edvLWvn   \n",
       "\n",
       "                           playlist_name  search_rank  \\\n",
       "558968                        motorcycle         1031   \n",
       "1309140                           charts         4762   \n",
       "1069690  rock nacional anos 90 anos 2000         1143   \n",
       "\n",
       "         bert_max_pln_similarity  bert_avg_pln_similarity  \\\n",
       "558968                   1.00000                 1.000000   \n",
       "1309140                  1.00000                 1.000000   \n",
       "1069690                  0.74047                 0.690026   \n",
       "\n",
       "         bert_max_token_max_similarity  bert_avg_token_max_similarity  \\\n",
       "558968                        1.000000                       1.000000   \n",
       "1309140                       1.000000                       1.000000   \n",
       "1069690                       0.841927                       0.770233   \n",
       "\n",
       "         bert_overall_avg_max_similarity  bert_overall_avg_avg_similarity  \\\n",
       "558968                          1.000000                          1.00000   \n",
       "1309140                         1.000000                          1.00000   \n",
       "1069690                         0.791198                          0.73013   \n",
       "\n",
       "         ...  ft_wiki_overall_avg_avg_similarity  ft_wiki_overall_similarity  \\\n",
       "558968   ...                            1.000000                    1.000000   \n",
       "1309140  ...                            1.000000                    1.000000   \n",
       "1069690  ...                            0.109992                    0.196221   \n",
       "\n",
       "         ft_crawl_max_pln_similarity  ft_crawl_avg_pln_similarity  \\\n",
       "558968                      1.000000                     1.000000   \n",
       "1309140                     1.000000                     1.000000   \n",
       "1069690                     0.319526                     0.258843   \n",
       "\n",
       "         ft_crawl_max_token_max_similarity  ft_crawl_avg_token_max_similarity  \\\n",
       "558968                            1.000000                           1.000000   \n",
       "1309140                           1.000000                           1.000000   \n",
       "1069690                           0.566329                           0.553025   \n",
       "\n",
       "         ft_crawl_overall_avg_max_similarity  \\\n",
       "558968                              1.000000   \n",
       "1309140                             1.000000   \n",
       "1069690                             0.442928   \n",
       "\n",
       "         ft_crawl_overall_avg_avg_similarity  ft_crawl_overall_similarity  \\\n",
       "558968                              1.000000                     1.000000   \n",
       "1309140                             1.000000                     1.000000   \n",
       "1069690                             0.405934                     0.424431   \n",
       "\n",
       "              proc_query  \n",
       "558968      [motorcycle]  \n",
       "1309140         [charts]  \n",
       "1069690  [90s, nineties]  \n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['w2v_max_token_max_similarity'], df['w2v_avg_token_max_similarity'] = zip(\n",
    "    *df.apply(lambda row: get_max_token_similarity(row['proc_query'],\n",
    "                                                   row['playlist_name'],\n",
    "                                                   row.name,\n",
    "                                                   get_w2v_similarity\n",
    "                                                  ), axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('proc_query', axis=1).reset_index(drop = True).to_feather('final_uniq_plns_WITH_BERT&USE&W2V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del m, w2v_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLoVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from warnings import filterwarnings\n",
    "# filterwarnings(\"ignore\", category = UserWarning)\n",
    "\n",
    "def get_glove_similarity(query: str, pln: str):\n",
    "    '''Calculate similarity using GLoVe using spaCy'''\n",
    "    return nlp(query).similarity(nlp(pln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large model: en_core_web_lg\n",
    "# vec for sim: en_vectors_web_lg\n",
    "nlp = spacy.load(\"en_vectors_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nAdding pln_similarity\\n')\n",
    "df['glove_max_pln_similarity'], df['glove_avg_pln_similarity'] = zip(\n",
    "    *df.apply(lambda row: get_pln_similarity(row['proc_query'],\n",
    "                                             row['playlist_name'],\n",
    "                                             row.name,\n",
    "                                             get_glove_similarity\n",
    "                                            ), axis = 1)\n",
    ")\n",
    "\n",
    "print(f'\\nAdding token_max_similarity\\n')\n",
    "df['glove_max_token_max_similarity'], df['glove_avg_token_max_similarity'] = zip(\n",
    "    *df.apply(lambda row: get_max_token_similarity(row['proc_query'],\n",
    "                                                   row['playlist_name'],\n",
    "                                                   row.name,\n",
    "                                                   get_glove_similarity\n",
    "                                                  ), axis = 1)\n",
    ")\n",
    "\n",
    "print(f'\\nAdding overall_similarity\\n')\n",
    "\n",
    "df = add_model_avg_similarity(df, 'glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>track_id</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>search_rank</th>\n",
       "      <th>bert_max_pln_similarity</th>\n",
       "      <th>bert_avg_pln_similarity</th>\n",
       "      <th>bert_max_token_max_similarity</th>\n",
       "      <th>bert_avg_token_max_similarity</th>\n",
       "      <th>bert_overall_avg_max_similarity</th>\n",
       "      <th>bert_overall_avg_avg_similarity</th>\n",
       "      <th>...</th>\n",
       "      <th>use_dan_overall_similarity</th>\n",
       "      <th>use_overall_similarity</th>\n",
       "      <th>proc_query</th>\n",
       "      <th>glove_max_pln_similarity</th>\n",
       "      <th>glove_avg_pln_similarity</th>\n",
       "      <th>glove_max_token_max_similarity</th>\n",
       "      <th>glove_avg_token_max_similarity</th>\n",
       "      <th>glove_overall_avg_max_similarity</th>\n",
       "      <th>glove_overall_avg_avg_similarity</th>\n",
       "      <th>glove_overall_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1050158</th>\n",
       "      <td>world</td>\n",
       "      <td>0Kjc4mrh7uoKPfotJGhvV1</td>\n",
       "      <td>alan jackson complete collection</td>\n",
       "      <td>466</td>\n",
       "      <td>0.784164</td>\n",
       "      <td>0.784164</td>\n",
       "      <td>0.890593</td>\n",
       "      <td>0.890593</td>\n",
       "      <td>0.837379</td>\n",
       "      <td>0.837379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367671</td>\n",
       "      <td>0.345408</td>\n",
       "      <td>[world]</td>\n",
       "      <td>0.381631</td>\n",
       "      <td>0.381631</td>\n",
       "      <td>0.329549</td>\n",
       "      <td>0.329549</td>\n",
       "      <td>0.355590</td>\n",
       "      <td>0.355590</td>\n",
       "      <td>0.355590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150884</th>\n",
       "      <td>musical</td>\n",
       "      <td>55do1f4mkLfM314tQDlyfw</td>\n",
       "      <td>chill acoustic music</td>\n",
       "      <td>981</td>\n",
       "      <td>0.853741</td>\n",
       "      <td>0.853741</td>\n",
       "      <td>0.904954</td>\n",
       "      <td>0.904954</td>\n",
       "      <td>0.879348</td>\n",
       "      <td>0.879348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568649</td>\n",
       "      <td>0.602636</td>\n",
       "      <td>[musical]</td>\n",
       "      <td>0.632764</td>\n",
       "      <td>0.632764</td>\n",
       "      <td>0.724151</td>\n",
       "      <td>0.724151</td>\n",
       "      <td>0.678457</td>\n",
       "      <td>0.678457</td>\n",
       "      <td>0.678457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352038</th>\n",
       "      <td>flute</td>\n",
       "      <td>5yaD6JCt7c0zvK4bSW5P11</td>\n",
       "      <td>flute</td>\n",
       "      <td>2304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[flute]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           query                track_id                     playlist_name  \\\n",
       "1050158    world  0Kjc4mrh7uoKPfotJGhvV1  alan jackson complete collection   \n",
       "1150884  musical  55do1f4mkLfM314tQDlyfw              chill acoustic music   \n",
       "1352038    flute  5yaD6JCt7c0zvK4bSW5P11                             flute   \n",
       "\n",
       "         search_rank  bert_max_pln_similarity  bert_avg_pln_similarity  \\\n",
       "1050158          466                 0.784164                 0.784164   \n",
       "1150884          981                 0.853741                 0.853741   \n",
       "1352038         2304                 1.000000                 1.000000   \n",
       "\n",
       "         bert_max_token_max_similarity  bert_avg_token_max_similarity  \\\n",
       "1050158                       0.890593                       0.890593   \n",
       "1150884                       0.904954                       0.904954   \n",
       "1352038                       1.000000                       1.000000   \n",
       "\n",
       "         bert_overall_avg_max_similarity  bert_overall_avg_avg_similarity  \\\n",
       "1050158                         0.837379                         0.837379   \n",
       "1150884                         0.879348                         0.879348   \n",
       "1352038                         1.000000                         1.000000   \n",
       "\n",
       "         ...  use_dan_overall_similarity  use_overall_similarity  proc_query  \\\n",
       "1050158  ...                    0.367671                0.345408     [world]   \n",
       "1150884  ...                    0.568649                0.602636   [musical]   \n",
       "1352038  ...                    1.000000                1.000000     [flute]   \n",
       "\n",
       "         glove_max_pln_similarity  glove_avg_pln_similarity  \\\n",
       "1050158                  0.381631                  0.381631   \n",
       "1150884                  0.632764                  0.632764   \n",
       "1352038                  1.000000                  1.000000   \n",
       "\n",
       "         glove_max_token_max_similarity  glove_avg_token_max_similarity  \\\n",
       "1050158                        0.329549                        0.329549   \n",
       "1150884                        0.724151                        0.724151   \n",
       "1352038                        1.000000                        1.000000   \n",
       "\n",
       "         glove_overall_avg_max_similarity  glove_overall_avg_avg_similarity  \\\n",
       "1050158                          0.355590                          0.355590   \n",
       "1150884                          0.678457                          0.678457   \n",
       "1352038                          1.000000                          1.000000   \n",
       "\n",
       "         glove_overall_similarity  \n",
       "1050158                  0.355590  \n",
       "1150884                  0.678457  \n",
       "1352038                  1.000000  \n",
       "\n",
       "[3 rows x 34 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1402119, 34)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(drop = True).drop('proc_query', 1).to_feather('final_uniq_plns_WITH_BERT&USE&W2V&GLOVE'); df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_similarity(query: str, pln: str):\n",
    "    '''Calculate similarity using FastText using Gensim'''\n",
    "    return m.similarity(query, pln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_en_path  = datapath('fasttext_pretrain/wiki_en.bin')\n",
    "m = load_facebook_vectors(wiki_en_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nAdding pln_similarity\\n')\n",
    "\n",
    "df['ft_wiki_max_pln_similarity'], df['ft_wiki_avg_pln_similarity'] = zip(\n",
    "    *df.apply(lambda row: get_pln_similarity(row['proc_query'],\n",
    "                                             row['playlist_name'],\n",
    "                                             row.name,\n",
    "                                             get_fasttext_similarity\n",
    "                                            ), axis = 1)\n",
    ")\n",
    "\n",
    "print(f'\\nAdding token_max_similarity\\n')\n",
    "\n",
    "df['ft_wiki_max_token_max_similarity'], df['ft_wiki_avg_token_max_similarity'] = zip(\n",
    "    *df.apply(lambda row: get_max_token_similarity(row['proc_query'],\n",
    "                                                   row['playlist_name'],\n",
    "                                                   row.name,\n",
    "                                                   get_fasttext_similarity\n",
    "                                                  ), axis = 1)\n",
    ")\n",
    "\n",
    "print(f'\\nAdding overall_similarity\\n')\n",
    "\n",
    "df = add_model_avg_similarity(df, 'ft_wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>track_id</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>search_rank</th>\n",
       "      <th>bert_max_pln_similarity</th>\n",
       "      <th>bert_avg_pln_similarity</th>\n",
       "      <th>bert_max_token_max_similarity</th>\n",
       "      <th>bert_avg_token_max_similarity</th>\n",
       "      <th>bert_overall_avg_max_similarity</th>\n",
       "      <th>bert_overall_avg_avg_similarity</th>\n",
       "      <th>...</th>\n",
       "      <th>glove_overall_avg_avg_similarity</th>\n",
       "      <th>glove_overall_similarity</th>\n",
       "      <th>proc_query</th>\n",
       "      <th>ft_wiki_max_pln_similarity</th>\n",
       "      <th>ft_wiki_avg_pln_similarity</th>\n",
       "      <th>ft_wiki_max_token_max_similarity</th>\n",
       "      <th>ft_wiki_avg_token_max_similarity</th>\n",
       "      <th>ft_wiki_overall_avg_max_similarity</th>\n",
       "      <th>ft_wiki_overall_avg_avg_similarity</th>\n",
       "      <th>ft_wiki_overall_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>975632</th>\n",
       "      <td>feeling good OR feeling great OR feeling happy</td>\n",
       "      <td>1rB5RVav3dKGJVwquqPXOK</td>\n",
       "      <td>good songs albums that will probably make me f...</td>\n",
       "      <td>1117</td>\n",
       "      <td>0.882575</td>\n",
       "      <td>0.817463</td>\n",
       "      <td>0.893242</td>\n",
       "      <td>0.830386</td>\n",
       "      <td>0.887909</td>\n",
       "      <td>0.823924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834370</td>\n",
       "      <td>0.845958</td>\n",
       "      <td>[feeling good, feeling great, feeling happy]</td>\n",
       "      <td>0.676448</td>\n",
       "      <td>0.610076</td>\n",
       "      <td>0.527879</td>\n",
       "      <td>0.493758</td>\n",
       "      <td>0.602163</td>\n",
       "      <td>0.551917</td>\n",
       "      <td>0.577040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333232</th>\n",
       "      <td>forgiveness</td>\n",
       "      <td>2GbCXMnXVdPwLyJLY6hjSM</td>\n",
       "      <td>david gray please forgive me</td>\n",
       "      <td>588</td>\n",
       "      <td>0.848999</td>\n",
       "      <td>0.848999</td>\n",
       "      <td>0.859454</td>\n",
       "      <td>0.859454</td>\n",
       "      <td>0.854227</td>\n",
       "      <td>0.854227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509171</td>\n",
       "      <td>0.509171</td>\n",
       "      <td>[forgiveness]</td>\n",
       "      <td>0.508786</td>\n",
       "      <td>0.508786</td>\n",
       "      <td>0.741231</td>\n",
       "      <td>0.741231</td>\n",
       "      <td>0.625009</td>\n",
       "      <td>0.625009</td>\n",
       "      <td>0.625009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387340</th>\n",
       "      <td>hypnotic</td>\n",
       "      <td>6bXSYxP6WDlNCr6Jk1H3MW</td>\n",
       "      <td>hypnotized youngboy never broke again</td>\n",
       "      <td>875</td>\n",
       "      <td>0.664611</td>\n",
       "      <td>0.664611</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.714815</td>\n",
       "      <td>0.714815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433202</td>\n",
       "      <td>0.433202</td>\n",
       "      <td>[hypnotic]</td>\n",
       "      <td>0.440686</td>\n",
       "      <td>0.440686</td>\n",
       "      <td>0.657368</td>\n",
       "      <td>0.657368</td>\n",
       "      <td>0.549027</td>\n",
       "      <td>0.549027</td>\n",
       "      <td>0.549027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 query  \\\n",
       "975632  feeling good OR feeling great OR feeling happy   \n",
       "333232                                     forgiveness   \n",
       "387340                                        hypnotic   \n",
       "\n",
       "                      track_id  \\\n",
       "975632  1rB5RVav3dKGJVwquqPXOK   \n",
       "333232  2GbCXMnXVdPwLyJLY6hjSM   \n",
       "387340  6bXSYxP6WDlNCr6Jk1H3MW   \n",
       "\n",
       "                                            playlist_name  search_rank  \\\n",
       "975632  good songs albums that will probably make me f...         1117   \n",
       "333232                       david gray please forgive me          588   \n",
       "387340              hypnotized youngboy never broke again          875   \n",
       "\n",
       "        bert_max_pln_similarity  bert_avg_pln_similarity  \\\n",
       "975632                 0.882575                 0.817463   \n",
       "333232                 0.848999                 0.848999   \n",
       "387340                 0.664611                 0.664611   \n",
       "\n",
       "        bert_max_token_max_similarity  bert_avg_token_max_similarity  \\\n",
       "975632                       0.893242                       0.830386   \n",
       "333232                       0.859454                       0.859454   \n",
       "387340                       0.765019                       0.765019   \n",
       "\n",
       "        bert_overall_avg_max_similarity  bert_overall_avg_avg_similarity  ...  \\\n",
       "975632                         0.887909                         0.823924  ...   \n",
       "333232                         0.854227                         0.854227  ...   \n",
       "387340                         0.714815                         0.714815  ...   \n",
       "\n",
       "        glove_overall_avg_avg_similarity  glove_overall_similarity  \\\n",
       "975632                          0.834370                  0.845958   \n",
       "333232                          0.509171                  0.509171   \n",
       "387340                          0.433202                  0.433202   \n",
       "\n",
       "                                          proc_query  \\\n",
       "975632  [feeling good, feeling great, feeling happy]   \n",
       "333232                                 [forgiveness]   \n",
       "387340                                    [hypnotic]   \n",
       "\n",
       "        ft_wiki_max_pln_similarity  ft_wiki_avg_pln_similarity  \\\n",
       "975632                    0.676448                    0.610076   \n",
       "333232                    0.508786                    0.508786   \n",
       "387340                    0.440686                    0.440686   \n",
       "\n",
       "        ft_wiki_max_token_max_similarity  ft_wiki_avg_token_max_similarity  \\\n",
       "975632                          0.527879                          0.493758   \n",
       "333232                          0.741231                          0.741231   \n",
       "387340                          0.657368                          0.657368   \n",
       "\n",
       "        ft_wiki_overall_avg_max_similarity  \\\n",
       "975632                            0.602163   \n",
       "333232                            0.625009   \n",
       "387340                            0.549027   \n",
       "\n",
       "        ft_wiki_overall_avg_avg_similarity  ft_wiki_overall_similarity  \n",
       "975632                            0.551917                    0.577040  \n",
       "333232                            0.625009                    0.625009  \n",
       "387340                            0.549027                    0.549027  \n",
       "\n",
       "[3 rows x 41 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1402119, 41)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(drop = True).drop('proc_query', 1).to_feather('final_uniq_plns_WITH_BERT&USE&W2V&GLOVE&FT'); df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcrawl_path = datapath('fasttext_pretrain/crawl-300d-2M-subword.bin')\n",
    "m = load_facebook_vectors(webcrawl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nAdding pln_similarity\\n')\n",
    "\n",
    "df['ft_crawl_max_pln_similarity'], df['ft_crawl_avg_pln_similarity'] = zip(\n",
    "    *df.apply(lambda row: get_pln_similarity(row['proc_query'],\n",
    "                                             row['playlist_name'],\n",
    "                                             row.name,\n",
    "                                             get_fasttext_similarity\n",
    "                                            ), axis = 1)\n",
    ")\n",
    "\n",
    "print(f'\\nAdding token_max_similarity\\n')\n",
    "\n",
    "df['ft_crawl_max_token_max_similarity'], df['ft_crawl_avg_token_max_similarity'] = zip(\n",
    "    *df.apply(lambda row: get_max_token_similarity(row['proc_query'],\n",
    "                                                   row['playlist_name'],\n",
    "                                                   row.name,\n",
    "                                                   get_fasttext_similarity\n",
    "                                                  ), axis = 1)\n",
    ")\n",
    "\n",
    "print(f'\\nAdding overall_similarity\\n')\n",
    "\n",
    "df = add_model_avg_similarity(df, 'ft_crawl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True).drop('proc_query', 1).to_feather('final_uniq_plns_WITH_BERT&USE&W2V&GLOVE&FT'); df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    " - Depending on how well an ensemble of these scores works to filter clean the data, I might also try:\n",
    "     - **ULMFiT**: fine-tuned with an entertainment / music Language Model, potentially Pitchfork or other music blogs\n",
    "     - **XLNet**: although the Semantic Similarity improvements from BERT to XLNet were negligible when introducing Whole Word Masking, and as of now, a strong performing uncased version isn't available, they did use a larger vocabulary to train the model so it might yield better results with user-gernerated data such as I have.\n",
    "     - **ELMo**: not really intended for this downstream task, but could be interesting to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
